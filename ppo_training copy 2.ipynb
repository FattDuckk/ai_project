{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e90438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "\n",
    "class RewardAndGoalLoggingCallback(BaseCallback):\n",
    "    def __init__(self, log_interval=20000, verbose=0):\n",
    "        super().__init__(verbose)\n",
    "        self.log_interval = log_interval\n",
    "        self.cumulative_reward = 0\n",
    "        self.step_counter = 0\n",
    "\n",
    "    def _on_step(self):\n",
    "        reward = self.locals[\"rewards\"][0]\n",
    "        self.cumulative_reward += reward\n",
    "        self.step_counter += 1\n",
    "        self.save_path=\"./ppo_checkpoints\"\n",
    "\n",
    "        if self.step_counter % self.log_interval == 0:\n",
    "            # Get the goal_pos from the environment (VecEnv-safe)\n",
    "            curent_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            goal_pos = self.training_env.get_attr(\"goal_pos\")[0]\n",
    "            # ef_pos = self.training_env.get_attr(\"robot\")[0].get_joint_obs()[\"ee_pos\"]\n",
    "            print(f\"time: {curent_time} ðŸ“ˆ Step {self.num_timesteps} | Reward (last {self.log_interval}): {round(self.cumulative_reward, 4)} | ðŸŽ¯ Goal: {np.round(goal_pos, 3)}\")\n",
    "            self.cumulative_reward = 0\n",
    "\n",
    "        # if self.num_timesteps % self.log_interval == 0:\n",
    "            path = f\"{self.save_path}/ppo_checkpoint_{self.num_timesteps}.zip\"\n",
    "            self.model.save(path)\n",
    "            # print(f\"ðŸ’¾ Model saved at step {self.num_timesteps} â†’ {path}\")\n",
    "\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "461bc1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pybullet build time: Jan 29 2025 23:16:28\n",
      "/home/fatduck/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â±ï¸ Timeout at Step 500 | EE: [0.08  0.067 0.303] | Goal: [-0.169 -0.151  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.131 -0.093  0.122] | Goal: [ 0.11  -0.321  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.094 -0.104  0.141] | Goal: [ 0.11  -0.254  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.114 -0.183  0.223] | Goal: [ 0.107 -0.32   0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.041 -0.041  0.28 ] | Goal: [ 0.107 -0.324  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.106 -0.225  0.161] | Goal: [ 0.106 -0.245  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.06  -0.175  0.065] | Goal: [-0.169 -0.151  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.181 -0.166  0.175] | Goal: [ 0.112 -0.252  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.165 -0.155 -0.003] | Goal: [ 0.109 -0.324  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.148 -0.118 -0.014] | Goal: [ 0.109 -0.317  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.026 -0.183  0.082] | Goal: [ 0.038 -0.319  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.004 -0.309  0.229] | Goal: [ 0.105 -0.32   0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.123 -0.277  0.017] | Goal: [ 0.115 -0.25   0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.005 -0.099  0.175] | Goal: [ 0.043 -0.321  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.154 -0.284  0.166] | Goal: [-0.169 -0.291  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.028 -0.044  0.278] | Goal: [ 0.11  -0.247  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.193 -0.139  0.03 ] | Goal: [ 0.045 -0.253  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.04  -0.132  0.131] | Goal: [ 0.107 -0.318  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.133 -0.176  0.019] | Goal: [ 0.112 -0.316  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.157 -0.128  0.004] | Goal: [ 0.112 -0.253  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.107 -0.119  0.132] | Goal: [ 0.037 -0.252  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.009 -0.154  0.228] | Goal: [ 0.109 -0.316  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.094 -0.189 -0.009] | Goal: [-0.169 -0.291  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.128 -0.133  0.205] | Goal: [ 0.114 -0.322  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.194 -0.204  0.093] | Goal: [-0.169 -0.221  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.024 -0.019  0.325] | Goal: [ 0.106 -0.321  0.06 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.08  -0.149  0.141] | Goal: [ 0.036 -0.247  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.164 -0.154  0.121] | Goal: [ 0.112 -0.315  0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.009 -0.023  0.33 ] | Goal: [ 0.109 -0.25   0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.116 -0.149 -0.002] | Goal: [-0.169 -0.291  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.148 -0.19   0.004] | Goal: [-0.169 -0.221  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.104 -0.129  0.106] | Goal: [ 0.042 -0.32   0.025]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.014 -0.231 -0.008] | Goal: [-0.169 -0.221  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [ 0.091 -0.075  0.143] | Goal: [ 0.04 -0.32  0.06]\n",
      "â±ï¸ Timeout at Step 500 | EE: [0.    0.003 0.344] | Goal: [-0.169 -0.221  0.03 ]\n",
      "â±ï¸ Timeout at Step 500 | EE: [-0.012 -0.302  0.024] | Goal: [ 0.043 -0.322  0.025]\n",
      "ðŸ“Š Step 20000 | âœ… Successes: 7 | â±ï¸ Timeouts: 36\n",
      "time: 2025-06-20 17:33:46 ðŸ“ˆ Step 20000 | Reward (last 20000): -5255.489 | ðŸŽ¯ Goal: [-0.169 -0.151  0.03 ]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'RewardAndGoalLoggingCallback' object has no attribute 'save_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m reward_logger \u001b[38;5;241m=\u001b[39m RewardAndGoalLoggingCallback(log_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20000\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Train with logging\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2_000_000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreward_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m env\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mppo_uarm_final_again\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:324\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 324\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n\u001b[1;32m    223\u001b[0m callback\u001b[38;5;241m.\u001b[39mupdate_locals(\u001b[38;5;28mlocals\u001b[39m())\n\u001b[0;32m--> 224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    225\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_info_buffer(infos, dones)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/callbacks.py:114\u001b[0m, in \u001b[0;36mBaseCallback.on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_calls \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnum_timesteps\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 27\u001b[0m, in \u001b[0;36mRewardAndGoalLoggingCallback._on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcumulative_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# if self.num_timesteps % self.log_interval == 0:\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_path\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/ppo_checkpoint_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39msave(path)\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# print(f\"ðŸ’¾ Model saved at step {self.num_timesteps} â†’ {path}\")\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RewardAndGoalLoggingCallback' object has no attribute 'save_path'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from env import UArmEnv\n",
    "\n",
    "# Create environment and model\n",
    "env = UArmEnv(render=False)\n",
    "model = PPO(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    ent_coef=0.005,        # Encourages broader exploration without overwhelming reward\n",
    "    n_steps=2048,          # Longer rollout â†’ better credit assignment\n",
    "    batch_size=256,        # Good match for long rollouts\n",
    "    gae_lambda=0.95,       # Slightly longer-term reward tracking\n",
    "    gamma=0.99,            # Standard long-term discount\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Add logging callback\n",
    "reward_logger = RewardAndGoalLoggingCallback(log_interval=20000)\n",
    "\n",
    "# Train with logging\n",
    "model.learn(total_timesteps=2_000_000, callback=reward_logger)\n",
    "\n",
    "env.close()\n",
    "\n",
    "model.save(\"ppo_uarm_final_again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0ee13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "\n",
    "model.save(\"ppo_uarm_overtrained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
